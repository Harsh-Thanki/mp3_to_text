1
00:00:00,000 --> 00:00:11,419
Okay, welcome everybody. This is public meeting number 42 of quiet happy quiet day. May 17, 2024.

2
00:00:12,779 --> 00:00:19,160
Gosh, we got a full agenda. We've got a speaker. I'm hoping he's going to join.

3
00:00:19,800 --> 00:00:30,039
How do we want to be? How do we want to be? Does. And just to just to ping him our drop-up message, Jim, is.

4
00:00:30,780 --> 00:00:35,579
Great. I'm going to start and we'll, we'll, we'll juggle the schedule if such it is late, but in the

5
00:00:35,579 --> 00:00:43,740
time, let's get started. Thank you all for joining a good bumper crop of members.

6
00:00:45,979 --> 00:00:51,700
Let me, let me tell you how membership rate is increasing. I think by the end of this month,

7
00:00:51,960 --> 00:00:58,960
we'll probably break 400 members where it about 370. Last night, I was at the fantastic event,

8
00:00:58,960 --> 00:01:08,560
a, I, T, P, a professional organization hosted an event. It was actually hosted by Accenture.

9
00:01:09,079 --> 00:01:16,260
So, a, I, T, P invited me as a speaker and there was another speaker, John Moran, John Moros, sorry.

10
00:01:17,040 --> 00:01:27,500
And it was downtown LA and I, that the talk was about AI and gosh, they had us there till about nine

11
00:01:27,500 --> 00:01:33,579
clock at night. The, the moderators at the, they stopped the questions. It, it went on a lot,

12
00:01:34,420 --> 00:01:42,959
quite a while. AI is a hot topic and look, we are gathered here as a group of critical thinkers.

13
00:01:43,239 --> 00:01:52,060
We're not simply cheer leaders for the status quo. This is a rich vein of intellect. And

14
00:01:53,180 --> 00:01:58,420
people that have got some sort of angst, not of a angst, we've got an angst and we think we know

15
00:01:58,420 --> 00:02:05,540
what we can do about it about how to build an organization that is a credible alternative to

16
00:02:05,540 --> 00:02:14,280
the monopolizing forces. So, thank you so much for participating and we're going to give you

17
00:02:14,280 --> 00:02:21,560
dates. Today, I think what we'll do is we'll go to the work group updates and then we will

18
00:02:24,520 --> 00:02:32,139
proceed, wait for our, our Lightning Talker, so, you're there to join and, and, and, and,

19
00:02:32,219 --> 00:02:40,360
and juggle the the order of events. Okay, so Debbie, back to you and let's let's get on with the agenda.

20
00:02:40,360 --> 00:02:46,960
Right, everybody, welcome, welcome. Well, what is up? You're next. So,

21
00:02:47,039 --> 00:02:51,580
I'm question like throw back to you. You're going to throw it back to me. Okay, so what I want to talk

22
00:02:51,580 --> 00:03:00,620
about is our beta program. Great, so I'm going to share an issue if you've not seen it already.

23
00:03:03,159 --> 00:03:08,060
So, I'm going to share the, I'm going to share the beta program. It's the second tab on the website.

24
00:03:09,240 --> 00:03:17,000
There we go. Now I'm up. Right, and the second tab on the website is, is the beta tab.

25
00:03:18,020 --> 00:03:26,120
And it says we're going to be running an AI beta program in Q3. And the, that's put the engineers

26
00:03:26,120 --> 00:03:31,819
at working on at the moment. All the furious coding of our volunteer engineers are to get

27
00:03:32,020 --> 00:03:41,379
the proof of concept that we showed at the end of Q1 up to a beta level. And what you're seeing here

28
00:03:41,379 --> 00:03:51,080
on the pages, the proof of concept, but the beta functionality will be enhanced. And

29
00:03:51,080 --> 00:04:01,639
the beta is going to run in academia, be a closed beta. So, we're running in schools, no,

30
00:04:01,639 --> 00:04:07,860
schools, although I did have some inbound queries about getting it to run in high schools.

31
00:04:08,280 --> 00:04:15,840
We might extend it in high schools, I'm not sure. So, if you scroll down here, the beta objectives

32
00:04:15,840 --> 00:04:25,319
for the host would be to bring cutting edge personally to your campus and to test personally

33
00:04:25,319 --> 00:04:32,100
AI in an academic setting. The two use cases, there's one is the virtual teachers assistant.

34
00:04:32,879 --> 00:04:40,019
So, here we have the Pi owner, the personal AI owner is a professor or some member of staff.

35
00:04:40,660 --> 00:04:46,399
The Pi users that they grant access to would be students. So, the student facing

36
00:04:47,159 --> 00:04:51,959
use case, the virtual teachers assistant. Ask the professor is a similar thing.

37
00:04:53,539 --> 00:05:01,839
And it need not be course curriculum material, it could be the body of writing, the body of research.

38
00:05:02,360 --> 00:05:08,660
It's in fact any knowledge base that the Pi owner is able to curate. And what we're making

39
00:05:09,560 --> 00:05:18,360
it easy to do is to create your own Pi to create your own knowledge base as well. And the knowledge base

40
00:05:19,840 --> 00:05:28,379
through the technology of Rage is very easy to curate. This is not the owner's retraining of a model.

41
00:05:30,000 --> 00:05:37,620
It is in fact a simple dragging and dropping of files into folders. So, that's that how easy it's going to be.

42
00:05:38,919 --> 00:05:50,340
The user objectives are to be able to interact with an AI in a meaningful way and get some value out of it.

43
00:05:51,660 --> 00:05:57,519
But for each Pi owner that releases it, let's say, to a student body of 100 students,

44
00:05:58,040 --> 00:06:02,480
each of those students will be able to click a button and say build my personal AI.

45
00:06:02,480 --> 00:06:09,180
So, what we want to do in this page is also test out the viability of the experience. Is that

46
00:06:09,180 --> 00:06:13,680
something people want to do? Do they want to build their own AI? If it's super simple, if it's three

47
00:06:13,680 --> 00:06:18,720
clicks, one click, what does it look like? Maybe it doesn't even have a face. Second click, what is

48
00:06:18,720 --> 00:06:24,560
it sound like? That seems to be important. Third click is what is the knowledge base that it's curated,

49
00:06:24,560 --> 00:06:38,180
that you're going to curate for your personal AI. It's a simple configuration, the

50
00:06:38,180 --> 00:06:42,920
server topology, well we've got the servers running up in AWS. What we're going to do is we're

51
00:06:42,920 --> 00:06:48,160
going to ask each university in college to host the backend for their own beta.

52
00:06:49,000 --> 00:06:56,019
It's a close beta to each college and access we manage through the domain emails,

53
00:06:56,980 --> 00:07:06,620
at least for the owners and then the Pi owners, let's staff faculty. So on, would release

54
00:07:06,620 --> 00:07:14,579
would grant access to students. Okay, so we've put up a

55
00:07:14,579 --> 00:07:20,659
apply to join the beta program, click there and we've already had five colleges sign up.

56
00:07:22,680 --> 00:07:28,979
And we hope to get more. If you want to bring this to your alma mater, let us know,

57
00:07:30,879 --> 00:07:36,019
get them to click, make an introduction. We've got some notable colleges. I think the first

58
00:07:36,019 --> 00:07:43,540
most enthusiastic one, and thanks to Aftion for bringing us, bringing us Cornell and so

59
00:07:43,540 --> 00:07:48,459
Cornell's the first one. But hey, there are a few others going to follow. I think we've got UC

60
00:07:48,459 --> 00:08:02,699
Riverside. We're speaking to Danza College. We've got lots of other inbound UCSD, Harvard,

61
00:08:03,579 --> 00:08:10,459
Dosh. Why don't we get Harvard? We've got Harvard through Bruchnaya and we built a bot for him

62
00:08:11,279 --> 00:08:18,459
and so look, we've not made a big noise about this yet. And once we've got a good

63
00:08:21,620 --> 00:08:27,839
sort of a forum of initial colleges, I think we can make a noise about that. And then the sort of

64
00:08:27,839 --> 00:08:35,200
foam of the other colleges are wanting to get involved. We'll obviously take effect. And it's our aim.

65
00:08:35,200 --> 00:08:46,899
It's our aim to get 20 to 30 colleges signed up. And we are also talking with potential

66
00:08:48,199 --> 00:08:54,159
infrastructure donors who want to participate in the beta program. And there seems to be enthusiasm

67
00:08:54,159 --> 00:09:03,340
for that. Not all colleges have the wherewithal to put up a number of these G5 servers.

68
00:09:04,480 --> 00:09:13,039
And so in that case, that hosting will be underwritten by a vendor of some sort of we talk

69
00:09:13,039 --> 00:09:21,120
into a few. And I want to thank Joe Sappers from SwissVolt for making introductions for us to

70
00:09:21,120 --> 00:09:29,240
there infrastructure partner. And we can take it forward from there. But hey, a lot more is needed.

71
00:09:29,240 --> 00:09:35,740
And so to make this successful, let's get that well covered. Okay, so

72
00:09:38,280 --> 00:09:45,559
I'm going to stop presenting their back to you, Debbie. Okay, one second.

73
00:09:51,640 --> 00:09:55,059
Okay, so next I think I believe we have Toby.

74
00:09:55,059 --> 00:09:58,620
Um, with hackathon.

75
00:10:01,559 --> 00:10:01,939
You hear me?

76
00:10:06,339 --> 00:10:08,379
Morning. Morning, Toby.

77
00:10:09,639 --> 00:10:22,840
So not much of these just lots and lots of positive energy and replies. I'm literally having that

78
00:10:22,840 --> 00:10:30,000
question to help you reply to the leads. So thank you. Um, we haven't converted. You need to convert.

79
00:10:30,259 --> 00:10:36,019
We need some dollars in the door. So that's the metric I'm looking at to bring back to

80
00:10:36,019 --> 00:10:43,820
hopefully that's what's all but yeah, which it's really turning up. Lots and lots and lots of people are.

81
00:10:44,320 --> 00:10:52,480
Oh, one big confirmation here is owner will happen. So, how is Nick? Thank you, Moose

82
00:10:52,480 --> 00:11:01,600
Apple. Arizona will happen as well. There's another super angel guy that came to the employee

83
00:11:01,600 --> 00:11:09,159
in the sub spot. I don't have to say the thank you as well. Um, so hackathon stuff is really moved with

84
00:11:09,159 --> 00:11:17,179
very fast. And if there's any other valid views that can help, I would love to talk to anyone

85
00:11:18,220 --> 00:11:23,820
who can help. And we can use mint toys as well. Yes, we need lots and lots of

86
00:11:23,820 --> 00:11:33,340
negatives. Oh, I'm Garble. I'm sorry. Positive, you guys. Is that better?

87
00:11:36,779 --> 00:11:39,440
Carson, you want to talk? Maybe you have the better direction.

88
00:11:48,519 --> 00:11:55,259
Well, he's getting ready. Hey, Toby, I have a that really famous guy, Khanol, in Orange County, California.

89
00:11:55,480 --> 00:12:01,240
Interesting and quiet and high. And what we're doing here for education and public schools.

90
00:12:01,580 --> 00:12:05,039
So, I just wanted to mention that and it's coming out to the show this weekend for the

91
00:12:05,039 --> 00:12:10,639
expose on Texas, meaning A. But yeah, I got someone who's really big and getting a lot of

92
00:12:10,639 --> 00:12:18,519
to come to us. So that's that. We back, Carson. This is considered next weekend for us on the show.

93
00:12:24,080 --> 00:12:37,659
Great. So cast and Toby last night. I met Frederick Whirl from his

94
00:12:37,659 --> 00:12:45,419
assistant dean of innovation at UCLA. And he said, yeah, we'd love to host your

95
00:12:45,419 --> 00:12:58,139
hackathon in June in Los Angeles at the UCLA Extension Building. So we've got that. And we also,

96
00:13:00,299 --> 00:13:05,740
we also made good friends with Accenture last night, because the event was hosted there.

97
00:13:08,399 --> 00:13:17,000
That was that was super important. We, I think there's there's capacity there for them to host.

98
00:13:17,440 --> 00:13:23,659
There's also capacity for them to sponsor. The next one you should probably follow up on is

99
00:13:24,259 --> 00:13:33,019
Joe Suppers on the show. Joe's on the call today from TwistVault, very eager to both sponsor.

100
00:13:33,019 --> 00:13:42,539
And maybe participate also in the hackathons either as infrastructure sponsors or

101
00:13:45,379 --> 00:13:50,759
they're not an open source developer, but there's ways in which they can participate

102
00:13:51,720 --> 00:14:03,639
through sponsorship. I, great. So I sent you those links and then we should follow up on that.

103
00:14:08,200 --> 00:14:12,940
Back to you. I mean, this is all you're funding.

104
00:14:14,100 --> 00:14:22,720
Okay. So what can I say in a public call? There's probably not a lot I can say in a public call,

105
00:14:22,899 --> 00:14:32,379
but we are approaching significant, we're getting significant activity regarding funding,

106
00:14:33,179 --> 00:14:43,580
all good news. And we are, as I say, doing the dance at the moment and trying to

107
00:14:46,659 --> 00:14:54,620
express our value as an organization. Okay. So there are a few things that

108
00:14:54,620 --> 00:15:02,960
naturally come up when funders, funders and sponsors engage with us. And it's the

109
00:15:02,960 --> 00:15:10,759
question of open source and our open source bona fide is and our purity to the open source

110
00:15:10,759 --> 00:15:18,279
way. And frankly, that's something we're debating internally. We're a community,

111
00:15:19,240 --> 00:15:27,200
of organizer, of of of developers, our community of critical thinkers. And I think that the

112
00:15:27,200 --> 00:15:36,159
community is the value that we have. Yeah, we're developing code as well. Anyone has access to the

113
00:15:36,159 --> 00:15:41,240
code that codes available in GitHub. You don't even have to sign a membership form.

114
00:15:41,279 --> 00:15:55,179
Um, in trying to explain this to a wide variety of individuals, I explain it and I think Debbie

115
00:15:55,179 --> 00:16:04,740
is also written something on this. That open source is like an open recipe for a meal. I'm going

116
00:16:04,740 --> 00:16:15,080
to show you this. This is a, this is a cookbook. This cookbook was our family cookbook. Our family cookbook

117
00:16:15,080 --> 00:16:23,360
that we developed during lockdown. It was a family lockdown project. And we got together the

118
00:16:23,360 --> 00:16:30,340
Russell family, the diaspora spread all over the world. During lockdown, we had Zoom calls and

119
00:16:30,340 --> 00:16:36,820
decided to share our family recipes. And in the sharing of the recipes, these recipes sometimes

120
00:16:36,820 --> 00:16:43,580
were held very privately. And I decided to share them and people then posted refinement to the recipes.

121
00:16:44,179 --> 00:16:51,039
And that was great because when the recipes were originally created, maybe a century ago,

122
00:16:51,659 --> 00:17:00,120
that was for a time. And that was for a time when certain ingredients were available.

123
00:17:00,120 --> 00:17:07,740
And as things evolved, it's important for recipes to become public and open source. Okay,

124
00:17:08,240 --> 00:17:14,740
so that is my metaphor for open source. The code is the recipe. It's not the product.

125
00:17:16,619 --> 00:17:24,460
You can go and clone the blockchain repo and get all the blockchain code. It doesn't mean you

126
00:17:24,460 --> 00:17:31,980
have the Bitcoin network or Ethereum or any of the other running products and services that are

127
00:17:31,980 --> 00:17:36,839
built on top of open source. So think about it like that. Now back to the recipe,

128
00:17:37,619 --> 00:17:45,859
metaphor, the recipe for a code you might implement inside a kitchen. Your kitchen's expensive.

129
00:17:46,259 --> 00:17:52,160
You know, it's expensive to operate. The fact that you've given someone your recipe doesn't mean they

130
00:17:52,160 --> 00:17:59,140
have access to your kitchen. Okay, where are these metaphors going? We've developed code for personal

131
00:17:59,140 --> 00:18:08,160
AI. We've got three repos, maybe four repos, one covering the front end, the agent. One covering

132
00:18:09,680 --> 00:18:16,079
the back end, the operating system. We have a kitchen as well. That is expensive to run.

133
00:18:16,079 --> 00:18:22,220
You know, we're paying the AWS bills over there. Doesn't mean everyone has access to it. So what

134
00:18:22,220 --> 00:18:29,220
we've done is we've created a sign up. The sign up is extremely light. Just give us your email address

135
00:18:29,220 --> 00:18:36,960
and commit that you can work eight hours a month on this project. If not, look, go and take the code

136
00:18:36,960 --> 00:18:44,839
and have added it. We'll even help you build the code. Bolleges done this. Bolleges helped people

137
00:18:44,839 --> 00:18:51,240
clone the repo and get the code running to the same level that we've got it running now,

138
00:18:51,319 --> 00:18:56,240
the proof of concept. But running in their own hardware, on their own infrastructure.

139
00:18:58,319 --> 00:19:04,519
So the membership that we ask people to sign up for is very light. There's no cost first of all. There's

140
00:19:05,779 --> 00:19:11,640
no contributor, license agreement that we've asked in people to sign up. But like your email,

141
00:19:11,640 --> 00:19:18,039
we'd like to know that you're going to stick around for a while. So commit eight hours. That is the

142
00:19:18,039 --> 00:19:27,740
barrier to entry. And so that is my metaphor for open source. It comes from this family cookbook

143
00:19:27,740 --> 00:19:35,380
that we built. You can go find it online. Love, letter to Lali. And Ruth's posted about it.

144
00:19:35,380 --> 00:19:40,000
I'm not being this to be a pitch for you to go and buy the cookbook.

145
00:19:40,900 --> 00:19:50,819
I'm I'm selling it in the chat. Okay. Open source is open recipe. It's not free food. It doesn't mean

146
00:19:50,819 --> 00:19:57,519
that you can come to my kitchen and cook. It doesn't mean that when I cook it or when you cook it,

147
00:19:57,880 --> 00:20:05,099
I can come to your house and sit at your table. It all it allows you to take the recipe and all the

148
00:20:05,659 --> 00:20:16,240
and go and open up a restaurant and sell it. That's also fine. If we choose to open up a restaurant,

149
00:20:16,799 --> 00:20:21,920
then we can charge you entrance and we can charge you like everyone else. You might have the recipe.

150
00:20:22,200 --> 00:20:28,160
You might have refined the recipe. We might be using your refinements to the recipe. But so that is

151
00:20:28,160 --> 00:20:36,339
the metaphor. I hope that is clear in everyone's mind. Now what is that quiet is building?

152
00:20:36,859 --> 00:20:45,319
Quiet is building that recipe. Okay. Great. But we also are running it. And what is emerging

153
00:20:45,319 --> 00:20:53,200
out of this and there's some fantastic work going on with I think Brian Ragazzi is

154
00:20:55,759 --> 00:21:06,819
exploring community inference. We've got several initiatives looking at community storage. Where is

155
00:21:06,819 --> 00:21:12,299
your personal AI going to be still? Where is your personal information going to be stored? And so we're

156
00:21:12,299 --> 00:21:19,480
looking at the solid protocol. But there are other protocols that are in play. There's the Apache

157
00:21:19,480 --> 00:21:26,019
ozone storage network. And there is the there are a number of others. So that is part of the

158
00:21:26,019 --> 00:21:33,299
discussion. So we're starting to see quite as a layered network. On top of the internet, imagine a

159
00:21:33,299 --> 00:21:41,160
storage fabric. On top of that, a community inference fabric. And then on top of that, the intent

160
00:21:41,160 --> 00:21:51,220
casting network. This is the implementation of DocSoul's book, the Intention Economy. Okay. And

161
00:21:51,220 --> 00:22:01,819
that network would be running standard protocols like the IEEE P7012 protocol. And hopefully

162
00:22:01,819 --> 00:22:09,160
the back end protocol doesn't look like our speakers going to join us today. So that's why I'm

163
00:22:09,240 --> 00:22:15,740
waffling on. And if he does then great. Okay. He might have his 10 minutes. But Sujitne was going to be

164
00:22:15,740 --> 00:22:24,420
on today talking about the back end protocol, which is this possible contenter for implementing

165
00:22:24,920 --> 00:22:32,039
the intent network. So the value is not in the code. The value is in the network. You can go

166
00:22:32,089 --> 00:22:39,410
and take the blockchain code, build that. It doesn't mean you have the Bitcoin network. So think

167
00:22:39,410 --> 00:22:46,609
about it like that. Be free with sharing the code. Don't worry about that. Don't worry about

168
00:22:46,609 --> 00:22:55,269
people coming in and picking the code and taking it off for themselves. That shouldn't concern us.

169
00:22:55,990 --> 00:23:03,470
I like the fact some licensed protocols say that if a third party that takes the code

170
00:23:03,470 --> 00:23:11,210
makes some improvement, they should feed that back. Yes, that's great. But that shouldn't be a

171
00:23:11,210 --> 00:23:23,089
stipulation. I think community is what is important. And that's what lives on top of the

172
00:23:23,089 --> 00:23:34,069
infrastructure network. So look, I've ran it on there a bit and hoping that Sujitne would

173
00:23:34,069 --> 00:23:46,930
turn up. I think what we should do is move the voices, the new member voices up and go straight

174
00:23:46,930 --> 00:23:52,829
there. And he shows up. We can always bring them into the next call.

175
00:23:53,529 --> 00:24:01,190
Absolutely. Yeah. Okay. So back to you, Debbie. I just wanted to say something that

176
00:24:01,190 --> 00:24:07,130
there are some of us here who are new to the open source community and we're learning how it

177
00:24:07,130 --> 00:24:13,250
works. And that's what I wrote. So you know a little bit of patience until we understand how

178
00:24:13,250 --> 00:24:19,569
it all works. But eventually we'll all come together. And you know Carson has shared some

179
00:24:19,569 --> 00:24:24,029
interesting links to read about it. I just put it out there because it's sort of like learning

180
00:24:24,029 --> 00:24:29,990
the basics of it. It explains it also as well. And so you know they want to share the link. That's

181
00:24:29,990 --> 00:24:36,690
fine. And new voices. Thank you, Riza. By the way, we're putting it out there because you know thanks.

182
00:24:38,730 --> 00:24:43,990
New voices. I hope me out with this because you know it is every single Friday.

183
00:24:44,650 --> 00:24:50,450
If you are new, please raise your hand. So we can start calling on you and ask that you introduce

184
00:24:50,450 --> 00:24:55,950
yourself. Tell us a little bit, you know, about a minute while you're here. What would you like to

185
00:24:55,950 --> 00:25:08,769
accomplish? So, Harsh, thank you, your turn. Hey, good morning, everyone. It's probably morning

186
00:25:08,769 --> 00:25:14,609
down there. But it's in Dubai. It's like 18 the night. Sorry, 9 30 of rocks. So I go to

187
00:25:14,609 --> 00:25:20,190
evening, whatever, whichever you prefer. First of all, I really nice meeting you all. I'm really

188
00:25:20,190 --> 00:25:27,470
excited. Joining this community. I know I came in touch with the idea of Kauai through Debbie.

189
00:25:27,529 --> 00:25:33,450
This is my best friend. And a big thank you to her to introducing me to the Kauai. And when I think

190
00:25:33,450 --> 00:25:40,589
about it, they play like, yeah, this would be like the way the air development is going on

191
00:25:40,589 --> 00:25:50,309
right now. The ability for the user to, you know, don't the user to make the user think that

192
00:25:50,309 --> 00:25:55,789
they don't have to worry about the data is really going to be a big topic in the future. So we are

193
00:25:55,789 --> 00:26:01,410
preparing it for tomorrow right now. So yeah, it's really good. And I would love to contribute

194
00:26:01,410 --> 00:26:07,849
to the code. And I'll do everything in my power to help you guys. Yeah, really good. Thank you. Thank you.

195
00:26:08,170 --> 00:26:09,509
Hi, thanks, Harsh. Thanks, Harsh.

196
00:26:11,730 --> 00:26:22,410
What's next? Ben Gold. Hello, everybody. I am an AI consultant from BaseDoubt of Dallas. I have

197
00:26:22,410 --> 00:26:29,490
known Resa through a common friend Darren who's also on this call. And what I do is I consult

198
00:26:29,490 --> 00:26:35,690
with different organizations on how to deploy responsible and ethical AI. So I work with

199
00:26:35,690 --> 00:26:42,009
high with schools, universities, as well with SaaS companies, startups. And the area that's really

200
00:26:42,009 --> 00:26:48,750
interesting is that I believe private AI is going to be something that every organization will need.

201
00:26:49,109 --> 00:26:56,369
That is either for privacy and for ethics. There are a lot of organizations where people are

202
00:26:56,369 --> 00:27:01,589
having their employees, they go get a personal chat GPT account, they leak company and

203
00:27:01,589 --> 00:27:06,630
permission into a private account. And it's really important for organizations to be able to

204
00:27:06,630 --> 00:27:13,049
leverage generative AI. But in a way that isn't compliance with company policies. And so that's

205
00:27:13,049 --> 00:27:17,390
what I'm really excited about. I think there's a lot of commercial uses because I believe

206
00:27:17,390 --> 00:27:23,769
every organization needs to have a secure private generative AI capability. And this is one of the ways

207
00:27:23,769 --> 00:27:31,490
to do that. Thank you Ben. Aman, you and I have something in common that my middle name is also

208
00:27:31,490 --> 00:27:41,109
YAL. So that's very cool. Yes it is. Go ahead. Okay, so my name is Amanya Al-Shafir. I'm a college student

209
00:27:41,109 --> 00:27:50,769
at UC Santa Cruz. I was invited by Toby and I also know Carson, Kirsten. Sorry about that. And yeah,

210
00:27:50,769 --> 00:27:56,450
in President of our cruise hacks as well as I am doing a startup which is pretty cool

211
00:27:57,269 --> 00:28:04,369
involved in health and medicine. So yeah, happy to be here. Thank you. We're happy to have you.

212
00:28:05,230 --> 00:28:20,750
And Prashun. You're muted Prashun. Hi. So I joined recently and I'm getting up to the speed trying to

213
00:28:20,750 --> 00:28:28,190
the code locally had some issues centered equals to SAM. But overall intention is to learn from

214
00:28:28,190 --> 00:28:35,589
here possibly contribute some good ideas, keep thinking and you know, see how it walks out. I'm

215
00:28:35,589 --> 00:28:46,089
very excited about the quiet project. Thanks Prashun. Thank you. And Kwanza. Good to everyone from Atlanta.

216
00:28:46,089 --> 00:28:50,650
Great to be on. Great to see you all. We're excited about the hackathon. Thanks for all the work

217
00:28:50,650 --> 00:28:55,269
behind the scenes. We're looking to try to round up some dollars. I had a great call with band

218
00:28:55,269 --> 00:29:01,269
Jones from Drain Machine and he said he's really excited about to connect with the group.

219
00:29:01,630 --> 00:29:05,869
Had no idea what we were doing and I told him we're trying to revolutionize this thing. So

220
00:29:05,869 --> 00:29:10,569
hey, Kudos to everybody that's chippin' in and doing all their part and let's keep pushing you

221
00:29:10,569 --> 00:29:15,529
all excited excited excited excited. Thank you Toby for having me here and getting me in the loop

222
00:29:15,529 --> 00:29:21,970
from Dubas Brack. Thank you. So, that's the former Congressman Kwanza Hall. Thank you so much for

223
00:29:21,970 --> 00:29:29,450
being on our call. Thanks Kwanza. Hey, Jane, it's got a whole lot of money. You can Jeff Beza. So that's a

224
00:29:29,450 --> 00:29:42,109
right. Thank you. And I believe Adam. I think Adam, you're muted.

225
00:29:45,509 --> 00:29:58,069
You're muted, Brad. Hey guys, you're here. So Adam Walker recently moved to the

226
00:29:58,069 --> 00:30:05,170
meeting by Toby. I worked for JP Morgan. I worked with an nonprofit. So just happy to be here

227
00:30:05,170 --> 00:30:11,089
as a fly on the wall to see what you guys are developing. A lot of which resonates and hope to learn.

228
00:30:15,190 --> 00:30:20,950
Thank you Adam. Thanks for showing up. Adam, pre-chains, you brush me too.

229
00:30:22,630 --> 00:30:32,589
And last, I won't have enough class, but Rafael. Hi, everybody. I have the pleasure of

230
00:30:32,589 --> 00:30:39,410
me and he was a data con, LA event last year where he was a keynote speaker, where he

231
00:30:39,410 --> 00:30:45,170
absolutely crashed. By the way, he is absolutely wonderful. And that's how I got involved. I actually

232
00:30:45,170 --> 00:30:52,529
got lots of love because I've just been caught up by Adam professor at USC Adamberg. And actually,

233
00:30:52,750 --> 00:31:01,490
just recently got a grant to from it's a, it's called the AI Spark grant, which is the Adamberg

234
00:31:01,490 --> 00:31:12,009
school and the USC cinematic school come together and try to connect AI and storytelling. So I'm

235
00:31:12,009 --> 00:31:19,009
coaching, which obviously a lot of privacy issues there. So very interesting to see how I can

236
00:31:19,009 --> 00:31:23,809
contribute to this community and thank you for the effort, everyone. Thanks, Rafael.

237
00:31:25,390 --> 00:31:34,309
Thanks, USC for hosting us again. Yeah, that's me time. Thank you. Yeah, the data con was good.

238
00:31:35,150 --> 00:31:41,329
Yeah. Actually, on that note, we are working with data con. I think USC said they can't host

239
00:31:41,990 --> 00:31:48,470
so we're looking for another venue and I think we're going to land UCLA. So, you know,

240
00:31:48,630 --> 00:31:57,950
cross town rivalry, maybe data con LA ends up at UCLA this year. We also, we've got an outstanding

241
00:31:57,950 --> 00:32:04,029
offer for it to come to college of the canyons where I'm out here in my wife's a professor. But hey,

242
00:32:04,029 --> 00:32:09,549
I think it's, I think it's UCLA would be a good catch if we can land that.

243
00:32:14,890 --> 00:32:20,829
So I'm going to call on you. I can see there's some new members that have not spoken for a while,

244
00:32:20,849 --> 00:32:34,009
either not spoken or not spoken for a while. So hands up because, you know, you know, it's going to

245
00:32:34,630 --> 00:32:35,150
great.

246
00:32:37,009 --> 00:32:44,309
AZ, go ahead. And AZ before you start, Christian, I wonder if you'd maybe also later on give us

247
00:32:44,309 --> 00:32:48,730
five minutes on what's going on in the email chapter. But AZ, if you go.

248
00:32:49,289 --> 00:32:55,569
Thank you, everyone. Early member and very interested in where we're going. I'm working on an open

249
00:32:55,569 --> 00:33:02,930
source startup project to use the E-Chang to reveal our known selves to us and become a distinction

250
00:33:02,930 --> 00:33:08,250
engineer or a self discovery mechanism. I have an app and I'm working on the next prototype.

251
00:33:09,809 --> 00:33:15,769
My, my concern right now for this platform is how do we know, like, is there a guarantee that

252
00:33:15,769 --> 00:33:22,329
we're never going to pull an open AI on our user base? You know, we're starting out as open source

253
00:33:22,329 --> 00:33:29,230
will we stay open source because my project will remain open source. So if I decide to use our tools

254
00:33:29,230 --> 00:33:37,269
here, can I have a, like is there any sort of decision or can we confidently say that we will not

255
00:33:37,269 --> 00:33:42,349
become close source? So whether we are for profit or not, is that a decision that is, you know,

256
00:33:42,390 --> 00:33:48,829
standing for all time or is it up for debate? It's, it's not something that is up for debate.

257
00:33:49,410 --> 00:33:56,150
A couple of months ago, we had a town hall meeting where we did debate at that point. The debates

258
00:33:57,110 --> 00:34:05,230
we, we posed three questions. Should, why indulge in commercial activity? Second question was,

259
00:34:05,650 --> 00:34:10,730
should quite just become a spec organization? I mean, just create the recipe. Don't create product.

260
00:34:12,250 --> 00:34:19,289
Should, why should volunteers participate in the upside of any activity, commercial activity?

261
00:34:20,090 --> 00:34:28,730
And the, the answer from the membership was yes, no, yes. So yes, we should indulge in commercial activity.

262
00:34:29,829 --> 00:34:35,369
No, we should not just become a passive spec organization. We should develop product. And yes,

263
00:34:37,429 --> 00:34:43,449
volunteers should participate in that commercial upside. So we haven't yet

264
00:34:45,270 --> 00:34:53,170
actualized the wishes of the membership yet. But here's what we don't want to do. We don't

265
00:34:53,170 --> 00:34:59,489
want to make the same mistakes of open AI. Those are glaringly. Those are horrible mistakes.

266
00:34:59,809 --> 00:35:06,690
They should really rename themselves closed AI. I think the last open source project they had was,

267
00:35:06,690 --> 00:35:11,650
I think, called Clipper or something like that. And that was like two years ago, since then,

268
00:35:11,650 --> 00:35:21,610
everything's been closed. And their motive is horribly for profit. And I'm surprised that the

269
00:35:21,610 --> 00:35:27,090
IRS has not gone after them. I know there's a, there's a, there's a kind of a private action

270
00:35:27,090 --> 00:35:34,409
between Elon Musk and, and then that's a whole separate drama going on. Let me talk a little bit

271
00:35:34,409 --> 00:35:41,690
about why they, they structurally broken is because they created a nonprofit. And then,

272
00:35:42,409 --> 00:35:50,949
internally, they created a wholly owned for profit that then started to suck all the oxygen out of

273
00:35:50,949 --> 00:36:01,230
the room and started to dictate their direction. There was a battle for power. You recall that.

274
00:36:03,150 --> 00:36:11,989
And that the, the, the, the four profit motives one out. Okay. So, well, look at where

275
00:36:11,989 --> 00:36:18,750
we're aware of that. We're aware of that. And I, particularly of surrounded myself with advisors,

276
00:36:19,449 --> 00:36:25,449
the board, our structure that we have and the people we have on the board would first

277
00:36:25,449 --> 00:36:33,230
or never allow me to do anything like that. I'm hoping. So, we need to keep ourselves honest.

278
00:36:36,150 --> 00:36:43,150
But we have a, and it's worth, it worth me actually willing on this point for a while.

279
00:36:43,670 --> 00:36:49,650
I'm going to share and I'm going to share a slide that talks about what our governing

280
00:36:49,650 --> 00:37:03,510
structure looks like. And it is, and it's important. And I'm, I'm glad that you asked this

281
00:37:03,510 --> 00:37:10,389
question because this is, this, this is a question even, you know, the, the funders are asking.

282
00:37:11,550 --> 00:37:18,549
If we, if we, if we, if we ally our brand to you, these are sponsors, questions, if we,

283
00:37:19,550 --> 00:37:25,329
are you going to, are you going to do a bait and switch? So, first of all, the emphasis that we're

284
00:37:25,329 --> 00:37:30,929
putting on this is to build a movement. The fact that we're executing the mission of the movement

285
00:37:30,929 --> 00:37:39,989
through technology is a secondary thing. You know, if, if, if we just done it through,

286
00:37:40,010 --> 00:37:46,269
I don't know, philosophizing, then then maybe there, it would be a different thing. But the fact is,

287
00:37:46,489 --> 00:37:53,510
we are, we are executing our mission through the things we know how to do, how we've identified

288
00:37:53,510 --> 00:38:02,530
there's a problem, the problem of monopolization of AI, by wealthy forces. And that increase

289
00:38:02,530 --> 00:38:10,369
monopolization in itself is dangerous for industry, it's dangerous for society. And we, we want to

290
00:38:10,369 --> 00:38:15,110
do something about it. In the ways we know how to do, because we're most, most of us are technologists.

291
00:38:15,630 --> 00:38:20,989
Here's, here's the governing structure. Here's, here's our membership, you know, people that have

292
00:38:20,989 --> 00:38:30,489
designed that agreement. And there's an advisory council. If you elected yourself as an advisor,

293
00:38:31,750 --> 00:38:36,849
you appointed yourself, you just nominated yourself as a advisor, then you're a part of this

294
00:38:36,849 --> 00:38:43,329
advisory council. Toby morning is the head of that advisory council. The business of this advisory

295
00:38:43,329 --> 00:38:51,110
council is to advise, it's to advise the executive offices, and to create propositions that

296
00:38:51,110 --> 00:38:58,070
gets submitted to the board of directors. The board of directors sit maybe quarterly, they're currently

297
00:38:58,070 --> 00:39:05,530
nine people on this board. So it's very difficult for it to radically sway in one direction or another.

298
00:39:05,550 --> 00:39:12,969
This is not like a C suite where you've got a dictatorial executive that that rule rules the

299
00:39:13,689 --> 00:39:20,090
this there's a governing board over here. And the board, he lex the offices, but also guides

300
00:39:20,090 --> 00:39:26,510
the offices by voting on these propositions. So look, it's a very similar sort of trichameral,

301
00:39:26,510 --> 00:39:33,969
you know, three three house type of governing structure where we've seen that before. And these type

302
00:39:33,969 --> 00:39:41,989
of structures could last hundreds of years. I mean, that's what we see. And yet we know we know

303
00:39:41,989 --> 00:39:47,809
even in our own government how these things can be can fall apart and become dysfunctional.

304
00:39:48,170 --> 00:39:54,650
And it's going to be important for us as a movement to be vigilant about that. So I think that's

305
00:39:54,650 --> 00:40:08,769
a really important question. The question about the other guiding ambition that we have is that

306
00:40:08,769 --> 00:40:15,009
we want to become self-sustaining. And a lot of open source developments don't have that

307
00:40:16,550 --> 00:40:22,349
that as their guiding principle to become self-sustaining. So what do I mean by that? I mean, we don't

308
00:40:22,349 --> 00:40:29,909
want to continuously go cap in hand asking for donations. That many open source developments that

309
00:40:29,909 --> 00:40:38,630
is their means of of existence. They go to corporations or donors or of other foundations. And they say

310
00:40:38,769 --> 00:40:44,769
please give us a donation so that we can continue to survive. And sometimes that comes to an end.

311
00:40:45,070 --> 00:40:51,050
And so you see a lot of open source developments, brilliant code, brilliant developers,

312
00:40:51,530 --> 00:40:57,130
but the code bases are now ghost towns. And GitHub is littered with that where their ghost towns

313
00:40:57,130 --> 00:41:03,489
where they didn't have sustainability as one of their core mission objectives. We want to be

314
00:41:03,570 --> 00:41:10,289
sustainable. In three to five years, I don't want to be going cap in hand asking for charity.

315
00:41:11,010 --> 00:41:19,929
We will not be going cap in a new year. Right. And so being that sort of prophetic charitable

316
00:41:19,929 --> 00:41:29,550
cause is not, it doesn't make us a credible viable opposition to them and upalising forces.

317
00:41:29,550 --> 00:41:36,289
Now we need to be massive. We need to grow our organization and our community. And the network

318
00:41:36,289 --> 00:41:43,429
should have so much value in it that the monopolising forces actually come to us and want to

319
00:41:43,429 --> 00:41:49,949
become part of the network and want to adopt the network. It's critically important that our

320
00:41:49,949 --> 00:41:54,949
guiding principles are actually built into our operating system. That's why I'm stickler

321
00:41:54,949 --> 00:42:00,210
calling this an operating system that we're building because the kernel of the operating system

322
00:42:00,210 --> 00:42:08,849
actually in shrines are guiding principles of self-servanty, of privacy, of security, but also of

323
00:42:08,849 --> 00:42:16,510
uplifting the individuals, uplifting humanity. Frankly, I'm not interested in the 10x improvement

324
00:42:16,510 --> 00:42:23,050
for an enterprise as profitability. I'm interested in the 10x improvement for individuals. I want AI

325
00:42:23,050 --> 00:42:30,269
to make me a better me. I want it to make you, or need to make you a better you. If the knock-on

326
00:42:30,269 --> 00:42:36,929
effect is you become a better employee and you're able to deliver more value for your company.

327
00:42:37,190 --> 00:42:42,929
You work for fantastic. I think that's going to be great. AC, that was a very long answer to a

328
00:42:42,929 --> 00:42:47,550
very short question. I hope it's important to me. Thank you. That's what I was looking for.

329
00:42:47,630 --> 00:42:52,630
I also wanted to mention, I'm a recent US citizen. So I don't have a stable job. I'm looking

330
00:42:52,630 --> 00:42:57,750
for work. If there is a network that we have for people who are interested in my

331
00:42:57,750 --> 00:43:04,289
renown, I'd love to know what we can do through. AC, AC, go to the job opportunities channel.

332
00:43:05,469 --> 00:43:12,010
There are a hundred plus jobs there with the hiring managers linked in contact.

333
00:43:13,390 --> 00:43:18,710
And just, I want to say this, if there are some really high, if you're a

334
00:43:18,710 --> 00:43:26,110
one of the problems with AI, my buddy is the hiring guy and magic.dea. They have multiple

335
00:43:26,110 --> 00:43:32,789
pigrices, but I want to tell you that hiring guy is okay. This place has been hard.

336
00:43:34,250 --> 00:43:39,030
So if you are that type of talent, please reach out to me because I can work with you just.

337
00:43:39,409 --> 00:43:46,550
Very good. Okay. And AC, he's a barrett booth. He's one of the top recruiters. He's on this

338
00:43:46,570 --> 00:43:53,869
all the network. Heading up on Slack and just have a chat with him and see where the opportunity

339
00:43:53,869 --> 00:43:59,989
is lying. Thank you so much. Very good. Welcome. Can you pick them in?

340
00:43:59,989 --> 00:44:13,789
Last again. Barrett booth. I think you'll find them on Slack. There we go.

341
00:44:17,349 --> 00:44:25,650
So I'm going to go through this. I'm going to call on you. I'm going to call on you.

342
00:44:25,650 --> 00:44:33,590
Could the well actually Christian? I know I, you might not have anything prepared, but it'd be

343
00:44:33,590 --> 00:44:39,170
great if you could just give us a few minutes from what's going on in the Emir chapter.

344
00:44:40,909 --> 00:44:53,530
Yes, I agree. Thank you. Okay. Holy is involved in and as you know next week, we will be

345
00:44:54,170 --> 00:45:03,590
a conference in Sicily. So we would be present meet, Ariesa and also Afshina. We will talk about AI

346
00:45:03,590 --> 00:45:11,730
and open science and the democratization process involved. So this will be a very good opportunity

347
00:45:11,730 --> 00:45:23,170
to to show our institution in order to let the graph in the Emir region. And I hope that

348
00:45:24,170 --> 00:45:28,469
we will we will have a lot of such as and we get involved with a lot of the

349
00:45:28,469 --> 00:45:36,989
person students and also institution. So this will be very challenging. For the rest of our communities

350
00:45:36,989 --> 00:45:48,150
in building, I want to say that next week I apologize, but I cannot attend the meeting. So we

351
00:45:48,150 --> 00:45:55,070
move to the next week. We will recover because to the conference I need to go one day before to prepare

352
00:45:55,070 --> 00:46:05,289
all and so I prefer to move our Emir meeting on the next meeting. Next week, sorry.

353
00:46:05,869 --> 00:46:13,070
So all this in progress, we are very strong. We grow up and I am very happy to join in this

354
00:46:13,070 --> 00:46:19,789
community to give my contribution. So take your ESA. I go back to you.

355
00:46:20,750 --> 00:46:32,789
Thanks. Okay, so in the list I see a few folks and you might have spoken already, but it's worth

356
00:46:32,789 --> 00:46:39,630
speaking up again. Darrell, I'm going to call on you. So Darrell, Serant,

357
00:46:41,750 --> 00:46:48,610
you're a new member but you become quite active. Perhaps you can talk about the project that

358
00:46:48,610 --> 00:47:01,829
that you're working on. All right, yeah. So I am working on a project to explore a new

359
00:47:02,549 --> 00:47:08,909
and we're exploring it. To see if there's a capabilities. So let me take a step back. So

360
00:47:08,909 --> 00:47:14,769
my mother is a new architecture that claims to be a little much more faster performance,

361
00:47:15,130 --> 00:47:20,369
influencing performance and possibly training performance than transformer-based

362
00:47:20,369 --> 00:47:27,510
architectures is what models like GPT-2, GPT-3, Lama, I'm kind of the more popular

363
00:47:27,510 --> 00:47:31,889
language models that are being used today. That's what they're based on. So I'm exploring

364
00:47:32,630 --> 00:47:40,309
Mamba, trying to understand hard works. And the goal is to hopefully build some new tools around

365
00:47:40,309 --> 00:47:46,030
Mamba. Maybe take a normal neural network and transform transform into a Mamba architecture.

366
00:47:47,150 --> 00:47:53,309
So if you're interested and help me out in this endeavor, probably going to be doing some

367
00:47:53,309 --> 00:48:01,989
work, I'm going to be doing move-off like benchmarking. There's some benchmarks and just

368
00:48:01,989 --> 00:48:05,690
just running some experiments. So if you're interested and help me out, feel free to just

369
00:48:05,690 --> 00:48:11,070
reach out to me on Slack or just on LinkedIn. I can just put my LinkedIn in the chat.

370
00:48:12,690 --> 00:48:16,590
I just want that's great. Thank you so much for sharing this thing. I've been bringing this

371
00:48:16,590 --> 00:48:21,050
one up for a while. It is where you're thinking of who could use it soon. I will connect with you

372
00:48:21,690 --> 00:48:28,610
Yeah. Absolutely. So look, this is important. Art of this, this came out of a fundamental AI research

373
00:48:28,610 --> 00:48:37,610
group. Thanks Darrell for diving into that area. Our mission is to try and get models to run

374
00:48:37,610 --> 00:48:48,349
to be smarter, faster and greener. Mamba actually claims to scale as N log N, whereas existing

375
00:48:48,349 --> 00:48:57,449
neural networks, deep neural networks scale, N squared. And so that N log N would be fantastic.

376
00:49:00,110 --> 00:49:07,210
The other guy who's not on the call, but I'm going to speak for him, metford,

377
00:49:08,190 --> 00:49:16,110
is working on a project of data curation. Data curation is super important in AI,

378
00:49:16,110 --> 00:49:21,090
garbage in garbage out. And some of the

379
00:49:21,830 --> 00:49:29,630
X that's going even into our rag databases is garbage because it comes from real time

380
00:49:29,630 --> 00:49:36,989
speech to text translation. And so there's a job for improving that. There's actually a job for

381
00:49:36,989 --> 00:49:45,929
curation of that data. Last night on the meeting with Ad Accenture, the speaker, the other speaker,

382
00:49:46,110 --> 00:49:51,809
was talked a lot about data curation and mechanisms for doing that. And metford is working on a

383
00:49:51,809 --> 00:49:58,750
project to do that. And if anyone's interested in participating on that project, contact metford.

384
00:50:00,090 --> 00:50:07,090
Great. Okay. I'm seeing a few others. Look, Darren Warner has not spoken for a while. Darren,

385
00:50:07,309 --> 00:50:12,789
I want you to maybe hop on and just give us an update on what you're working on.

386
00:50:12,789 --> 00:50:19,670
Here is a, yeah, I apologize. I've been you know, crazy busy with my day job recently. So don't have

387
00:50:19,670 --> 00:50:29,989
a huge update, but I think since the last time the ice spoke on here, we got the the three AI

388
00:50:29,989 --> 00:50:36,369
engines, Bruce Edu, and a fact actually moved over to the frock from your personal account.

389
00:50:36,369 --> 00:50:45,050
So the quiet org, AWS accounts, it's still largely a manual setup. We've got the framework,

390
00:50:45,809 --> 00:50:50,590
the cloud automation itself is automated, but the installation of the software is still very

391
00:50:50,590 --> 00:50:56,869
much a manual process. As soon as I can find some free time, I want to try and assist with that.

392
00:50:57,869 --> 00:51:05,289
And you know, I've always been an advocate of hands off what they call CICD environments. So I hope

393
00:51:05,289 --> 00:51:10,849
we can get there. And I hope we can get to a point where anyone can just, you know, if they've

394
00:51:10,849 --> 00:51:16,070
got a powerful enough probably gaming computer, whatever it might be, it's just to be able to download

395
00:51:16,070 --> 00:51:19,650
and install software and start playing around with it and stuff. I'm certainly really looking for

396
00:51:19,650 --> 00:51:26,130
to do that. And I'm going to get, I'm going to get my my son's interested in in building PCs.

397
00:51:26,190 --> 00:51:31,670
So I think I'm going to get him involved in in building me something that's capable to of

398
00:51:31,670 --> 00:51:37,989
running these engines. Fantastic. And look, Darren, I'm already promising the colleges and

399
00:51:37,989 --> 00:51:41,789
universities that I'm speaking with our IT departments. They say, hey, what do we need? Well,

400
00:51:41,969 --> 00:51:45,590
it's this machine. Yeah, but how do we, how do we run out of this? Don't worry, there's going to be

401
00:51:45,590 --> 00:51:51,789
a one-click DevOps recipe that you're going to be able to click and it's going to light it up,

402
00:51:51,789 --> 00:52:01,130
either in your infrastructure or in your AWS account. So let's make it so great. So

403
00:52:01,670 --> 00:52:08,630
the other guy I'm going to call on is Brian Regazzi. Let me tell you, first of all, what Darren's

404
00:52:08,630 --> 00:52:17,329
describing is what we want to do for the base of phase where there is a server that's going to run

405
00:52:17,329 --> 00:52:26,250
the agents and that's great. But we're thinking of what is the network going to look like beyond that.

406
00:52:26,250 --> 00:52:34,030
And we anticipate this fabric, as I said, a storage fabric, a community inference fabric,

407
00:52:34,750 --> 00:52:41,190
and then the intent casting network on top of it. But what if Brian, sorry to put you on the

408
00:52:41,190 --> 00:52:47,510
spot, but I wonder if you just speak up and talk about the community inference fabric? Sure.

409
00:52:48,090 --> 00:52:59,110
Okay. So we're working on sort of doing a proof of concept of using a technology called,

410
00:52:59,110 --> 00:53:03,550
well, so there's already something out there called pedals. You can see it at pedals.dev,

411
00:53:04,329 --> 00:53:08,390
where they have a number of what they call swarms, where basically you can take your

412
00:53:10,050 --> 00:53:17,989
your GPU and make it participate in a larger, in a larger group. There's been, there's been a lot

413
00:53:17,989 --> 00:53:24,389
of advancements in being able to distribute the processing of a model over a bunch of different

414
00:53:24,389 --> 00:53:29,650
GPUs instead of just having it to run it on one gigantic GPU that forces you to use like the

415
00:53:29,650 --> 00:53:37,170
big expensive cloud providers and so on. So they've got these swarms and you can connect to them

416
00:53:37,170 --> 00:53:44,949
and lend your GPU to processing of, you know, inference of models, also training. That's another

417
00:53:44,949 --> 00:53:54,030
later issue that we can get into that potentially is even more exciting. And so under the hood,

418
00:53:54,329 --> 00:53:59,869
it uses a, it uses a technology called hive mind and that's something you can pretty easily find out

419
00:53:59,869 --> 00:54:09,630
as well. And that's just a library that'll work with PyTorch and some others and allows you to

420
00:54:11,409 --> 00:54:19,630
to federate together a bunch of systems. So what we're looking to do is to create a, you know,

421
00:54:19,989 --> 00:54:25,130
the equivalent of a swarm for a quiet, where, you know, anybody who wants to participate,

422
00:54:25,130 --> 00:54:33,090
to join in and, and, and, and, and as a group, we probably have a lot of computing power available.

423
00:54:33,949 --> 00:54:37,989
So that that opens up a lot of possibilities for for the organization.

424
00:54:39,670 --> 00:54:53,309
Great. Thank you so much, Brian. Okay. And we, we had at the peak 57 members. That's pretty good.

425
00:54:55,050 --> 00:54:59,630
I'm going to, I'm going to, let's see if Doc, Doc, you want to speak up.

426
00:55:08,590 --> 00:55:16,429
We, let's, okay. So anybody that wants to work on, on this with, with Brian, reach out to Brian.

427
00:55:16,429 --> 00:55:24,269
There is, in fact, a, a Slack channel called distributed computing, but we'll probably,

428
00:55:25,069 --> 00:55:31,670
actually set up a work group around this, I think it's, it's more important. And then just

429
00:55:31,670 --> 00:55:38,829
having a discussion group and we'll probably set up work group meetings. Let's see if,

430
00:55:41,029 --> 00:55:45,150
okay, you're in a place where you can talk, I can, I can, I can do it. I just, I, I had the

431
00:55:45,150 --> 00:55:50,630
mute on and I couldn't find, I have so many links open and this is in a link and a tab, rather.

432
00:55:52,230 --> 00:55:54,989
I'm, I'm not sure, I much to say at this point, I'm just trying to help you with,

433
00:55:56,969 --> 00:56:01,610
positioning and funding and, and that kind of thing at this point. Doc, you posted,

434
00:56:02,409 --> 00:56:07,969
blog post on the difference between personalized and personal. I wonder if you can just riff on that.

435
00:56:07,969 --> 00:56:15,530
Sure, um, all of the, I mean, nearly all far as I know, um, everything we're getting from

436
00:56:15,530 --> 00:56:23,349
opening, I met a, um, Google the rest of them is personalized. It's not, it's not personal in the sense

437
00:56:23,349 --> 00:56:30,769
that it's ours. Uh, I compare that to being, it's like 1974, we don't have PCs yet. Um,

438
00:56:31,030 --> 00:56:35,409
I was not only alive, but an adult with two kids at that point. So, but I remember that,

439
00:56:35,409 --> 00:56:41,869
if you said personal computing or a personal computer back then, it was absurd. It was like saying

440
00:56:41,869 --> 00:56:48,489
personal nuclear power plant. Um, and, oh, I don't know what probably 99.99,

441
00:56:48,849 --> 00:56:54,849
x percent of the investment going into AI is going into corporate AI. It's going into the assumption

442
00:56:55,590 --> 00:57:02,309
that you are just a user and they can know more about you than you need to, every, they'll take

443
00:57:02,309 --> 00:57:08,750
everything. And, um, I think probably Apple is the only outlier there. We don't know what the

444
00:57:08,750 --> 00:57:12,070
hell Apple is doing and whatever they're going to end up doing is going to be somewhere in their

445
00:57:12,070 --> 00:57:20,670
closed and close to, um, wall garden. So, um, we're in a unique position here. We, we had, we,

446
00:57:20,789 --> 00:57:25,170
far as I know, and I looked at it and I put it in that in that piece. Um,

447
00:57:25,809 --> 00:57:31,230
collides the only one that's coming from from the individual that's doing that is about our sovereignty.

448
00:57:31,929 --> 00:57:37,849
It's, we can do for, for the entire marketplace, the entire world with PCs different computing,

449
00:57:38,590 --> 00:57:44,510
which is far more, um, effective and capable and transformative and revolutionary

450
00:57:44,510 --> 00:57:49,469
than anything the giants are doing, even though what we're getting right now from the giants

451
00:57:49,469 --> 00:57:54,889
is fabulous. It's wonderful. It's doing all kinds of cool stuff. Um, but if you're thinking about AI

452
00:57:54,889 --> 00:57:59,590
entirely inside the enterprise, you're missing it and you're missing what we can do here. So,

453
00:57:59,789 --> 00:58:05,650
that's, uh, you know, that's sort of what I'm trying to, you know, train it out. Okay,

454
00:58:05,650 --> 00:58:10,869
good. Why say to the world, right? So that's, uh, that's where that's at. But I'll put the link

455
00:58:10,869 --> 00:58:18,889
into the thing. Thanks so much, Doc. Okay, look, on the garden. On that note, um, I think what we'll

456
00:58:18,889 --> 00:58:24,309
do is we'll we'll move on the, the, the call straight after this is the idea exchange. I have

457
00:58:24,309 --> 00:58:31,050
someone can post the link to that. If you know, if you don't have it already, um, chance, that

458
00:58:31,050 --> 00:58:42,550
will be, that would be fantastic. I see. Yeah, I think it's posted in the general channel and, um,

459
00:58:43,010 --> 00:58:49,829
great. It's, we've, we've, we've got it here in the, in the thread. So, um, thanks so much. Thanks. So,

460
00:58:49,829 --> 00:58:56,030
there's meeting number 42. Great. And, um, and thanks for everybody for coming together.

461
00:58:56,930 --> 00:59:05,090
And, uh, having a happy, quiet day, uh, wish you all a great weekend. And, um, it's a gloomy,

462
00:59:05,090 --> 00:59:13,429
Los Angeles. Let's hope it brightens up, um, today. But, um, we're going to, uh, move over now to

463
00:59:14,550 --> 00:59:17,190
the, uh, idea exchange. Prashun, you got your hand up?

464
00:59:17,989 --> 00:59:21,150
Oh, no, no, I should ask, don't forget the thumbs up. Okay.

465
00:59:21,329 --> 00:59:27,469
Okay. The thumbs up. Exactly. There we go. Um, great. Thank you very much.

466
00:59:27,889 --> 00:59:31,110
Thank you everybody. Have a great weekend. Thank you, bye.

467
00:59:31,570 --> 00:59:32,849
Bye. Bye. Bye.

